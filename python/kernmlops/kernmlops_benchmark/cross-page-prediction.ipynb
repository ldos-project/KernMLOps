{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:03:12.054841: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 17:03:12.606995: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 17:03:13.182833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744736593.580866 1211236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744736593.729790 1211236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744736594.797732 1211236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744736594.797767 1211236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744736594.797771 1211236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744736594.797774 1211236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 17:03:14.919325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Attention, Concatenate\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_data(file_path, source_name):\n",
    "    \"\"\"Load a parquet file and return a DataFrame\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: {source_name} file not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading {source_name} data from: {file_path}\")\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"{source_name} data shape: {df.shape}\")\n",
    "    print(f\"{source_name} columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Convert object columns to numeric if possible\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                print(f\"Converted column {col} from object to numeric\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert column {col} to numeric: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(df, time_col='time_sec'):\n",
    "    \"\"\"Normalize timestamps to start from zero.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    min_time = df[time_col].min()\n",
    "    if pd.isna(min_time):\n",
    "        print(f\"Warning: No valid timestamps in {time_col}\")\n",
    "        return df\n",
    "    df[f'{time_col}_normalized'] = df[time_col] - min_time\n",
    "    print(f\"Normalized timestamps: min={min_time}, range=[0, {df[time_col].max() - min_time}]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rss_data(rss_df, page_size_mb):\n",
    "    \"\"\"Process RSS data with appropriate page size conversion\"\"\"\n",
    "    if rss_df is None or rss_df.empty:\n",
    "        print(\"Warning: RSS DataFrame is empty\")\n",
    "        return None, []\n",
    "\n",
    "    # Convert timestamp from ns to sec\n",
    "    if 'ts_ns' in rss_df.columns:\n",
    "        rss_df['time_sec'] = rss_df['ts_ns'] / 1e9\n",
    "        print(\"Converted timestamp from ns to sec\")\n",
    "\n",
    "    # Sort by timestamp\n",
    "    rss_df = rss_df.sort_values('time_sec')\n",
    "\n",
    "    # Normalize timestamps to start from 0\n",
    "    rss_df = normalize_timestamps(rss_df)\n",
    "\n",
    "    # Add page size as a feature\n",
    "    rss_df['page_size_mb'] = page_size_mb\n",
    "\n",
    "    # Identify target columns (memory metrics)\n",
    "    rss_cols = []\n",
    "    for col in ['anon', 'file', 'swap', 'shmem']:\n",
    "        if col in rss_df.columns:\n",
    "            if rss_df[col].std() > 0:  # Check for variance\n",
    "                rss_cols.append(col)\n",
    "                print(f\"Found RSS column with variance: {col}\")\n",
    "\n",
    "    # Convert page counts to MB using the correct page size\n",
    "    for col in rss_cols:\n",
    "        new_col = f\"{col}_mb\"\n",
    "        rss_df[new_col] = rss_df[col] * page_size_mb\n",
    "        print(f\"Converted {col} to MB using {page_size_mb}MB page size\")\n",
    "\n",
    "    return rss_df, rss_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tlb_data(tlb_df, tlb_type):\n",
    "    \"\"\"Process TLB miss data\"\"\"\n",
    "    if tlb_df is None or tlb_df.empty:\n",
    "        print(f\"Warning: {tlb_type} DataFrame is empty\")\n",
    "        return None, []\n",
    "\n",
    "    # Convert timestamp to seconds\n",
    "    if 'ts_uptime_us' in tlb_df.columns:\n",
    "        tlb_df['time_sec'] = tlb_df['ts_uptime_us'] / 1e6\n",
    "        print(f\"Converted {tlb_type} timestamp from us to sec\")\n",
    "\n",
    "    # Sort by timestamp\n",
    "    tlb_df = tlb_df.sort_values('time_sec')\n",
    "\n",
    "    # Normalize timestamps to start from 0\n",
    "    tlb_df = normalize_timestamps(tlb_df)\n",
    "\n",
    "    # Find TLB miss columns\n",
    "    tlb_cols = []\n",
    "    for col in tlb_df.columns:\n",
    "        if 'miss' in col.lower():\n",
    "            if tlb_df[col].std() > 0:  # Check for variance\n",
    "                tlb_cols.append(col)\n",
    "    print(f\"Found {len(tlb_cols)} {tlb_type} columns with variance\")\n",
    "\n",
    "    # Choose main TLB miss column (prefer cumulative/total)\n",
    "    main_tlb_col = None\n",
    "    for col in tlb_cols:\n",
    "        if 'cumulative' in col.lower() or 'total' in col.lower():\n",
    "            main_tlb_col = col\n",
    "            break\n",
    "\n",
    "    if main_tlb_col is None and tlb_cols:\n",
    "        main_tlb_col = tlb_cols[0]\n",
    "\n",
    "    if main_tlb_col:\n",
    "        print(f\"Selected {tlb_type} column: {main_tlb_col}\")\n",
    "        tlb_df[f'{tlb_type}_misses'] = tlb_df[main_tlb_col]\n",
    "\n",
    "        # Add derivative features\n",
    "        tlb_df[f'{tlb_type}_misses_rate'] = tlb_df[f'{tlb_type}_misses'].diff() / tlb_df['time_sec_normalized'].diff()\n",
    "        tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n",
    "\n",
    "        # Add rolling average\n",
    "        tlb_df[f'{tlb_type}_misses_avg'] = tlb_df[f'{tlb_type}_misses'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "        return tlb_df, [f'{tlb_type}_misses', f'{tlb_type}_misses_rate', f'{tlb_type}_misses_avg']\n",
    "    else:\n",
    "        print(f\"Warning: No suitable {tlb_type} column found\")\n",
    "        return tlb_df, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_change_dataset(kb2_data, kb4_data):\n",
    "    \"\"\"\n",
    "    Create a dataset that aligns memory changes with the most recent TLB measurements.\n",
    "\n",
    "    Returns:\n",
    "      - A dictionary with integrated datasets and a list of available memory metrics.\n",
    "    \"\"\"\n",
    "    print(\"Creating memory-change aligned dataset...\")\n",
    "\n",
    "    # Extract DataFrames\n",
    "    kb2_rss = kb2_data.get('rss')\n",
    "    kb2_dtlb = kb2_data.get('dtlb')\n",
    "    kb2_itlb = kb2_data.get('itlb')\n",
    "\n",
    "    kb4_rss = kb4_data.get('rss')\n",
    "    kb4_dtlb = kb4_data.get('dtlb')\n",
    "    kb4_itlb = kb4_data.get('itlb')\n",
    "\n",
    "    if kb2_rss is None or kb4_rss is None:\n",
    "        raise ValueError(\"Both 2MB and 4KB RSS data are required\")\n",
    "\n",
    "    print(f\"2MB RSS time range: {kb2_rss['time_sec_normalized'].min()} to {kb2_rss['time_sec_normalized'].max()}\")\n",
    "    print(f\"4KB RSS time range: {kb4_rss['time_sec_normalized'].min()} to {kb4_rss['time_sec_normalized'].max()}\")\n",
    "\n",
    "    if kb2_dtlb is not None:\n",
    "        print(f\"2MB DTLB time range: {kb2_dtlb['time_sec_normalized'].min()} to {kb2_dtlb['time_sec_normalized'].max()}\")\n",
    "    if kb2_itlb is not None:\n",
    "        print(f\"2MB ITLB time range: {kb2_itlb['time_sec_normalized'].min()} to {kb2_itlb['time_sec_normalized'].max()}\")\n",
    "    if kb4_dtlb is not None:\n",
    "        print(f\"4KB DTLB time range: {kb4_dtlb['time_sec_normalized'].min()} to {kb4_dtlb['time_sec_normalized'].max()}\")\n",
    "    if kb4_itlb is not None:\n",
    "        print(f\"4KB ITLB time range: {kb4_itlb['time_sec_normalized'].min()} to {kb4_itlb['time_sec_normalized'].max()}\")\n",
    "\n",
    "    print(\"Checking for duplicate timestamps...\")\n",
    "    def deduplicate_timestamps(df, time_col='time_sec_normalized'):\n",
    "        if df is None:\n",
    "            return None\n",
    "        before_count = len(df)\n",
    "        if df[time_col].duplicated().any():\n",
    "            print(f\"  Found duplicate timestamps in {time_col}; removing duplicates...\")\n",
    "            df = df.sort_values(time_col).drop_duplicates(subset=[time_col], keep='last')\n",
    "            print(f\"  Removed {before_count - len(df)} duplicates\")\n",
    "        return df\n",
    "\n",
    "    for name, df in [('2MB RSS', kb2_rss), ('2MB DTLB', kb2_dtlb), ('2MB ITLB', kb2_itlb),\n",
    "                     ('4KB RSS', kb4_rss), ('4KB DTLB', kb4_dtlb), ('4KB ITLB', kb4_itlb)]:\n",
    "        if df is not None:\n",
    "            df = deduplicate_timestamps(df)\n",
    "            if name == '2MB RSS': kb2_rss = df\n",
    "            elif name == '2MB DTLB': kb2_dtlb = df\n",
    "            elif name == '2MB ITLB': kb2_itlb = df\n",
    "            elif name == '4KB RSS': kb4_rss = df\n",
    "            elif name == '4KB DTLB': kb4_dtlb = df\n",
    "            elif name == '4KB ITLB': kb4_itlb = df\n",
    "\n",
    "    print(\"Identifying memory change points...\")\n",
    "    def find_memory_changes(df, memory_cols, threshold=0.01):\n",
    "        if df is None:\n",
    "            return pd.DataFrame()\n",
    "        change_points = []\n",
    "        for col in memory_cols:\n",
    "            col_mb = f\"{col}_mb\"\n",
    "            if col_mb in df.columns:\n",
    "                diff = df[col_mb].diff().abs()\n",
    "                changes = df[diff > threshold].copy()\n",
    "                if not changes.empty:\n",
    "                    changes['memory_metric'] = col_mb\n",
    "                    changes['memory_value'] = changes[col_mb]\n",
    "                    change_points.append(changes[['time_sec_normalized', 'memory_metric', 'memory_value']])\n",
    "        if not change_points:\n",
    "            return pd.DataFrame()\n",
    "        result = pd.concat(change_points).sort_values('time_sec_normalized')\n",
    "        return result\n",
    "\n",
    "    kb2_memory_candidates = [col for col in ['anon', 'file'] if col in kb2_rss.columns]\n",
    "    kb4_memory_candidates = [col for col in ['anon', 'file'] if col in kb4_rss.columns]\n",
    "\n",
    "    kb2_changes = find_memory_changes(kb2_rss, kb2_memory_candidates)\n",
    "    kb4_changes = find_memory_changes(kb4_rss, kb4_memory_candidates)\n",
    "\n",
    "    print(f\"Found {len(kb2_changes)} memory change points in 2MB data\")\n",
    "    print(f\"Found {len(kb4_changes)} memory change points in 4KB data\")\n",
    "\n",
    "    print(\"Aligning memory changes with TLB measurements using merge_asof...\")\n",
    "\n",
    "    def align_with_tlb(changes_df, dtlb_df, itlb_df):\n",
    "        if changes_df is None or changes_df.empty:\n",
    "            return changes_df\n",
    "\n",
    "        result = changes_df.copy()\n",
    "\n",
    "        if dtlb_df is not None:\n",
    "            dtlb_df = dtlb_df.sort_values('time_sec_normalized')\n",
    "            dtlb_columns = [col for col in dtlb_df.columns\n",
    "                           if 'time_sec' not in col and ('miss' in col.lower() or 'rate' in col.lower() or 'avg' in col.lower())]\n",
    "\n",
    "            result = pd.merge_asof(\n",
    "                result,\n",
    "                dtlb_df[['time_sec_normalized'] + dtlb_columns],\n",
    "                on='time_sec_normalized',\n",
    "                direction='backward',\n",
    "                suffixes=(\"\", \"_dtlb\")\n",
    "            )\n",
    "\n",
    "        if itlb_df is not None:\n",
    "            itlb_df = itlb_df.sort_values('time_sec_normalized')\n",
    "            itlb_columns = [col for col in itlb_df.columns\n",
    "                           if 'time_sec' not in col and ('miss' in col.lower() or 'rate' in col.lower() or 'avg' in col.lower())]\n",
    "\n",
    "            result = pd.merge_asof(\n",
    "                result,\n",
    "                itlb_df[['time_sec_normalized'] + itlb_columns],\n",
    "                on='time_sec_normalized',\n",
    "                direction='backward',\n",
    "                suffixes=(\"\", \"_itlb\")\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "    kb2_df = align_with_tlb(kb2_changes, kb2_dtlb, kb2_itlb)\n",
    "    kb4_df = align_with_tlb(kb4_changes, kb4_dtlb, kb4_itlb)\n",
    "\n",
    "    print(f\"Created 2MB integrated dataset with {len(kb2_df)} rows and {len(kb2_df.columns)} columns\")\n",
    "    print(f\"Created 4KB integrated dataset with {len(kb4_df)} rows and {len(kb4_df.columns)} columns\")\n",
    "\n",
    "    kb2_memory_metrics = kb2_df['memory_metric'].unique().tolist() if not kb2_df.empty else []\n",
    "    kb4_memory_metrics = kb4_df['memory_metric'].unique().tolist() if not kb4_df.empty else []\n",
    "\n",
    "    print(f\"2MB memory metrics available: {kb2_memory_metrics}\")\n",
    "    print(f\"4KB memory metrics available: {kb4_memory_metrics}\")\n",
    "\n",
    "    return {\n",
    "        'kb2_df': kb2_df,\n",
    "        'kb4_df': kb4_df,\n",
    "        'kb2_memory_metrics': kb2_memory_metrics,\n",
    "        'kb4_memory_metrics': kb4_memory_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regular_intervals(kb2_df, kb4_df, n_bins=100):\n",
    "    \"\"\"\n",
    "    Create regular time intervals for both datasets to align them on a\n",
    "    common time scale regardless of actual times.\n",
    "    \"\"\"\n",
    "    print(f\"Creating {n_bins} regular time intervals...\")\n",
    "\n",
    "    if kb2_df.empty or kb4_df.empty:\n",
    "        print(\"Warning: One or both datasets are empty\")\n",
    "        return kb2_df, kb4_df, None\n",
    "\n",
    "    # Get the range of normalized timestamps for both datasets\n",
    "    kb2_max_time = kb2_df['time_sec_normalized'].max()\n",
    "    kb4_max_time = kb4_df['time_sec_normalized'].max()\n",
    "\n",
    "    print(f\"2MB max normalized time: {kb2_max_time}\")\n",
    "    print(f\"4KB max normalized time: {kb4_max_time}\")\n",
    "\n",
    "    # Create bins that cover both datasets\n",
    "    max_time = max(kb2_max_time, kb4_max_time)\n",
    "    bin_edges = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    # Assign bins to each dataset\n",
    "    kb2_df['time_bin'] = pd.cut(kb2_df['time_sec_normalized'], bins=bin_edges, labels=False)\n",
    "    kb4_df['time_bin'] = pd.cut(kb4_df['time_sec_normalized'], bins=bin_edges, labels=False)\n",
    "\n",
    "    # Filter out NaN bins (points outside bin range)\n",
    "    kb2_valid = kb2_df.dropna(subset=['time_bin'])\n",
    "    kb4_valid = kb4_df.dropna(subset=['time_bin'])\n",
    "\n",
    "    print(f\"Valid 2MB points after binning: {len(kb2_valid)}/{len(kb2_df)}\")\n",
    "    print(f\"Valid 4KB points after binning: {len(kb4_valid)}/{len(kb4_df)}\")\n",
    "\n",
    "    return kb2_valid, kb4_valid, bin_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cross_config_data(integrated_data, target_metric=None, window_size=1.0, step_size=0.5):\n",
    "    \"\"\"\n",
    "    Prepare data for training a cross-configuration model using a sliding window approach.\n",
    "    \n",
    "    Uses the integrated 4KB data filtered by the specified memory metric as the target\n",
    "    and the 2MB integrated data as features. Also extracts 2MB RSS for visualization.\n",
    "    \n",
    "    Args:\n",
    "        integrated_data: Dictionary with integrated datasets.\n",
    "        target_metric: Memory metric to use as target (e.g., 'anon_mb').\n",
    "        window_size: Size of the sliding window in seconds.\n",
    "        step_size: Step size for the sliding window in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with aligned data ready for sequence preparation.\n",
    "    \"\"\"\n",
    "    kb2_df = integrated_data.get('kb2_df')\n",
    "    kb4_df = integrated_data.get('kb4_df')\n",
    "    \n",
    "    if kb2_df is None or kb2_df.empty or kb4_df is None or kb4_df.empty:\n",
    "        raise ValueError(\"Both 2MB and 4KB integrated data are required and must not be empty\")\n",
    "    \n",
    "    available_metrics = kb4_df['memory_metric'].unique()\n",
    "    if target_metric is None:\n",
    "        target_metric = available_metrics[0] if len(available_metrics) > 0 else None\n",
    "        print(f\"No target metric specified. Using first available: {target_metric}\")\n",
    "    else:\n",
    "        if target_metric not in available_metrics:\n",
    "            raise ValueError(f\"Target metric {target_metric} not found in 4KB data. Available: {available_metrics}\")\n",
    "    \n",
    "    print(f\"Using memory metric {target_metric} as target column\")\n",
    "    kb4_targets = kb4_df[kb4_df['memory_metric'] == target_metric][['time_sec_normalized', 'memory_value']].copy()\n",
    "    if kb4_targets.empty:\n",
    "        raise ValueError(f\"No rows found for target metric {target_metric} in 4KB data\")\n",
    "    kb4_targets = kb4_targets.rename(columns={'memory_value': '4kb_target'})\n",
    "    print(f\"Extracted {len(kb4_targets)} target rows from 4KB data\")\n",
    "    \n",
    "    # Extract 2MB RSS for visualization (same metric as target)\n",
    "    kb2_rss = kb2_df[kb2_df['memory_metric'] == target_metric][['time_sec_normalized', 'memory_value']].copy()\n",
    "    kb2_rss = kb2_rss.rename(columns={'memory_value': '2mb_rss'})\n",
    "    print(f\"Extracted {len(kb2_rss)} 2MB RSS rows for metric {target_metric}\")\n",
    "    \n",
    "    # Sort both datasets by time\n",
    "    kb4_targets = kb4_targets.sort_values('time_sec_normalized')\n",
    "    kb2_df = kb2_df.sort_values('time_sec_normalized')\n",
    "    kb2_rss = kb2_rss.sort_values('time_sec_normalized')\n",
    "    \n",
    "    # Define sliding window parameters\n",
    "    min_time = min(kb4_targets['time_sec_normalized'].min(), kb2_df['time_sec_normalized'].min())\n",
    "    max_time = max(kb4_targets['time_sec_normalized'].max(), kb2_df['time_sec_normalized'].max())\n",
    "    if pd.isna(min_time) or pd.isna(max_time):\n",
    "        raise ValueError(\"Invalid time range detected in datasets\")\n",
    "    window_starts = np.arange(min_time, max_time - window_size + step_size, step_size)\n",
    "    \n",
    "    print(f\"Creating sliding windows: window_size={window_size}s, step_size={step_size}s\")\n",
    "    print(f\"Time range: {min_time} to {max_time}, {len(window_starts)} windows\")\n",
    "    \n",
    "    # Initialize lists to store windowed data\n",
    "    window_data = []\n",
    "    kb2_features = [col for col in kb2_df.columns \n",
    "                   if col not in ['time_sec_normalized', 'memory_metric', 'memory_value']]\n",
    "    \n",
    "    for start in window_starts:\n",
    "        end = start + window_size\n",
    "        # Filter 2MB data within the window\n",
    "        kb2_window = kb2_df[(kb2_df['time_sec_normalized'] >= start) & \n",
    "                           (kb2_df['time_sec_normalized'] < end)]\n",
    "        # Filter 4KB target within the window\n",
    "        kb4_window = kb4_targets[(kb4_targets['time_sec_normalized'] >= start) & \n",
    "                                (kb4_targets['time_sec_normalized'] < end)]\n",
    "        # Filter 2MB RSS within the window\n",
    "        kb2_rss_window = kb2_rss[(kb2_rss['time_sec_normalized'] >= start) & \n",
    "                                (kb2_rss['time_sec_normalized'] < end)]\n",
    "        \n",
    "        # Aggregate 2MB features (mean within the window)\n",
    "        if not kb2_window.empty:\n",
    "            kb2_agg = kb2_window[kb2_features].mean().to_dict()\n",
    "        else:\n",
    "            # If no 2MB data, fill with NaN\n",
    "            kb2_agg = {col: np.nan for col in kb2_features}\n",
    "        \n",
    "        # Get 4KB target (use the last value in the window if available)\n",
    "        if not kb4_window.empty:\n",
    "            kb4_target = kb4_window['4kb_target'].iloc[-1]\n",
    "        else:\n",
    "            kb4_target = np.nan\n",
    "        \n",
    "        # Get 2MB RSS (use the last value in the window if available)\n",
    "        if not kb2_rss_window.empty:\n",
    "            kb2_rss_value = kb2_rss_window['2mb_rss'].iloc[-1]\n",
    "        else:\n",
    "            kb2_rss_value = np.nan\n",
    "        \n",
    "        # Store the window data\n",
    "        window_data.append({\n",
    "            'window_start': start,\n",
    "            'window_end': end,\n",
    "            'time_sec': (start + end) / 2,  # Use midpoint for reference\n",
    "            '4kb_target': kb4_target,\n",
    "            '2mb_rss': kb2_rss_value,\n",
    "            **kb2_agg\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    cross_config_df = pd.DataFrame(window_data)\n",
    "    \n",
    "    # Drop rows where the target is missing\n",
    "    initial_rows = len(cross_config_df)\n",
    "    cross_config_df = cross_config_df.dropna(subset=['4kb_target'])\n",
    "    print(f\"Dropped {initial_rows - len(cross_config_df)} rows with missing 4KB targets\")\n",
    "    \n",
    "    if cross_config_df.empty:\n",
    "        raise ValueError(\"No valid data remains after dropping rows with missing 4KB targets\")\n",
    "    \n",
    "    # Check for NaN values in features\n",
    "    nan_counts = cross_config_df[kb2_features].isna().sum()\n",
    "    print(f\"NaN counts in features:\\n{nan_counts}\")\n",
    "    cross_config_df[kb2_features] = cross_config_df[kb2_features].fillna(0)  # Fill NaNs with 0 for features only\n",
    "    \n",
    "    print(f\"Final cross-configuration dataset has {len(cross_config_df)} rows and {len(cross_config_df.columns)} columns\")\n",
    "    \n",
    "    return cross_config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sequences(X, y, noise_level=0.05, n_augmentations=3):\n",
    "    \"\"\"\n",
    "    Augment training data with slightly noisy versions to increase dataset size.\n",
    "\n",
    "    Args:\n",
    "        X: Input sequences\n",
    "        y: Target values\n",
    "        noise_level: Standard deviation of noise to add\n",
    "        n_augmentations: Number of augmented sequences to create per original\n",
    "\n",
    "    Returns:\n",
    "        Augmented X and y arrays\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting {len(X)} sequences with {n_augmentations} variations (noise level: {noise_level})...\")\n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "\n",
    "    # Add small random noise to create new samples\n",
    "    for i in range(n_augmentations):\n",
    "        noise = np.random.normal(0, noise_level, X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack([X_aug, X_noisy])\n",
    "        y_aug = np.vstack([y_aug, y])\n",
    "\n",
    "    print(f\"After augmentation: {len(X_aug)} sequences\")\n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_sequences(df, target_col, feature_cols, seq_length=10):\n",
    "    \"\"\"\n",
    "    Prepare sequences for the LSTM model.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with aligned data.\n",
    "        target_col: Target column name.\n",
    "        feature_cols: List of feature column names.\n",
    "        seq_length: Sequence length.\n",
    "\n",
    "    Returns:\n",
    "        X, y, scalers, valid_feature_cols.\n",
    "    \"\"\"\n",
    "    print(f\"Preparing sequences with target: {target_col}\")\n",
    "    print(f\"Using {len(feature_cols)} features\")\n",
    "    valid_feature_cols = []\n",
    "    for col in feature_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].std() > 0:\n",
    "                valid_feature_cols.append(col)\n",
    "            else:\n",
    "                print(f\"Skipping constant column: {col}\")\n",
    "    print(f\"Using {len(valid_feature_cols)} valid features after filtering\")\n",
    "\n",
    "    if not valid_feature_cols:\n",
    "        raise ValueError(\"No valid feature columns with non-zero variance\")\n",
    "\n",
    "    scalers = {}\n",
    "    scaled_data = {}\n",
    "    for col in valid_feature_cols + [target_col]:\n",
    "        try:\n",
    "            scaler = MinMaxScaler()\n",
    "            data = df[col].values.reshape(-1, 1)\n",
    "            if np.all(np.isnan(data)):\n",
    "                raise ValueError(f\"Column {col} contains only NaN values\")\n",
    "            scaled_data[col] = scaler.fit_transform(data)\n",
    "            scalers[col] = scaler\n",
    "        except Exception as e:\n",
    "            print(f\"Error scaling {col}: {e}\")\n",
    "            if col in valid_feature_cols:\n",
    "                valid_feature_cols.remove(col)\n",
    "\n",
    "    if not scalers:\n",
    "        raise ValueError(\"No columns could be scaled successfully\")\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        features_seq = []\n",
    "        for col in valid_feature_cols:\n",
    "            features_seq.append(scaled_data[col][i:i+seq_length])\n",
    "        X.append(np.hstack(features_seq))\n",
    "        y.append(scaled_data[target_col][i+seq_length])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], seq_length, len(valid_feature_cols)))\n",
    "    print(f\"Created {len(X)} sequences with shape {X.shape}\")\n",
    "\n",
    "    # Check for NaN values in sequences\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"NaN values detected in prepared sequences\")\n",
    "\n",
    "    return X, y, scalers, valid_feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_page_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build a model to predict 4KB memory usage from 2MB metrics.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Tuple with input shape (sequence_length, n_features).\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    print(f\"Building model with input shape: {input_shape}\")\n",
    "\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # LSTM layers with attention\n",
    "    lstm1 = LSTM(64, activation='relu', return_sequences=True)(inputs)\n",
    "    lstm1 = Dropout(0.2)(lstm1)\n",
    "\n",
    "    lstm2 = LSTM(32, activation='relu', return_sequences=True)(lstm1)\n",
    "    lstm2 = Dropout(0.2)(lstm2)\n",
    "\n",
    "    # Self-attention mechanism\n",
    "    attention = Attention()([lstm2, lstm2])\n",
    "\n",
    "    # Final LSTM layer\n",
    "    lstm_out = LSTM(16, activation='relu')(attention)\n",
    "    lstm_out = Dropout(0.2)(lstm_out)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1)(lstm_out)\n",
    "\n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(X, y, input_shape, n_splits=5, epochs=30, batch_size=16):\n",
    "    \"\"\"\n",
    "    Perform time-series cross-validation for the model.\n",
    "\n",
    "    Args:\n",
    "        X: Input sequences\n",
    "        y: Target values\n",
    "        input_shape: Shape for model input\n",
    "        n_splits: Number of cross-validation folds\n",
    "        epochs: Training epochs per fold\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        List of metrics for each fold and the final trained model\n",
    "    \"\"\"\n",
    "    print(f\"Performing {n_splits}-fold time series cross-validation...\")\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    best_model = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Add data augmentation for training set\n",
    "        X_train_aug, y_train_aug = augment_sequences(X_train, y_train, noise_level=0.03)\n",
    "\n",
    "        # Build and train model\n",
    "        model = build_cross_page_model(input_shape)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_aug, y_train_aug,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        if np.any(np.isnan(y_pred)):\n",
    "            print(f\"Warning: NaN predictions in fold {fold+1}\")\n",
    "            continue\n",
    "        mse = np.mean(np.square(y_pred - y_test))\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        print(f\"Fold {fold+1} - MSE: {mse:.4f}, RMSE: {rmse:.4f}\")\n",
    "        fold_metrics.append({\n",
    "            'fold': fold+1,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'train_samples': len(X_train_aug),\n",
    "            'test_samples': len(X_test)\n",
    "        })\n",
    "\n",
    "        # Save the best model\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model = model\n",
    "\n",
    "    if not fold_metrics:\n",
    "        raise ValueError(\"No valid folds completed due to prediction issues\")\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_mse = np.mean([m['mse'] for m in fold_metrics])\n",
    "    avg_rmse = np.mean([m['rmse'] for m in fold_metrics])\n",
    "    print(f\"\\nAverage MSE across folds: {avg_mse:.4f}\")\n",
    "    print(f\"Average RMSE across folds: {avg_rmse:.4f}\")\n",
    "\n",
    "    return fold_metrics, best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(actual_values, predicted_values, kb2_rss_values, time_steps=None, save_path='output/prediction_comparison.png'):\n",
    "    \"\"\"\n",
    "    Create detailed visualizations comparing predicted vs actual 4KB memory usage, including 2MB RSS.\n",
    "\n",
    "    Args:\n",
    "        actual_values: Array of actual 4KB memory values\n",
    "        predicted_values: Array of predicted 4KB memory values\n",
    "        kb2_rss_values: Array of 2MB RSS values\n",
    "        time_steps: Optional array of time points for x-axis (if None, will use indices)\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    # Make sure inputs are numpy arrays\n",
    "    actual = np.array(actual_values).flatten()\n",
    "    predicted = np.array(predicted_values).flatten()\n",
    "    kb2_rss = np.array(kb2_rss_values).flatten()\n",
    "\n",
    "    if np.all(np.isnan(actual)) or np.all(np.isnan(predicted)):\n",
    "        print(f\"Warning: All actual or predicted 4KB values are NaN. Skipping visualization at {save_path}\")\n",
    "        return {\n",
    "            'mse': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'mae': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'correlation': np.nan\n",
    "        }\n",
    "\n",
    "    # Create x-axis (either time steps or indices)\n",
    "    x_values = time_steps if time_steps is not None else np.arange(len(actual))\n",
    "    x_label = 'Time (seconds)' if time_steps is not None else 'Sample Index'\n",
    "\n",
    "    # Calculate error metrics\n",
    "    error = actual - predicted\n",
    "    mse = np.nanmean(np.square(error))\n",
    "    rmse = np.sqrt(mse) if not np.isnan(mse) else np.nan\n",
    "    mae = np.nanmean(np.abs(error))\n",
    "    mape = np.nanmean(np.abs(error / (actual + 1e-10))) * 100\n",
    "    r2 = 1 - (np.nansum(np.square(error)) / np.nansum(np.square(actual - np.nanmean(actual)))) if not np.all(np.isnan(actual)) else np.nan\n",
    "    corr = np.corrcoef(actual, predicted)[0, 1] if not (np.all(np.isnan(actual)) or np.all(np.isnan(predicted))) else np.nan\n",
    "\n",
    "    # Create figure with multiple subplots\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # 1. Main comparison plot\n",
    "    plt.subplot(3, 2, (1, 2))\n",
    "    plt.plot(x_values, actual, 'b-', linewidth=2, label='Actual 4KB Memory')\n",
    "    plt.plot(x_values, predicted, 'r--', linewidth=2, label='Predicted 4KB Memory')\n",
    "    plt.plot(x_values, kb2_rss, 'g-.', linewidth=2, label='Actual 2MB Memory')\n",
    "    plt.title('Memory Usage Comparison: 4KB (Actual vs Predicted) and 2MB', fontsize=16)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel('Memory Usage (MB)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Add metrics as text annotation\n",
    "    metrics_text = f\"MSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nMAPE: {mape:.2f}%\\nR²: {r2:.4f}\"\n",
    "    plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                 fontsize=12)\n",
    "\n",
    "    # 2. Error plot\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.bar(x_values, error, color='purple', alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "    plt.title('Prediction Error (Actual - Predicted 4KB)', fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel('Error (MB)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Scatter plot\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.scatter(actual, predicted, alpha=0.7, c='blue')\n",
    "    # Add scatter for 2MB vs 4KB actual\n",
    "    plt.scatter(kb2_rss, actual, alpha=0.7, c='green', marker='^', label='2MB vs 4KB Actual')\n",
    "\n",
    "    # Add perfect prediction line\n",
    "    valid_actual = actual[~np.isnan(actual) & ~np.isnan(predicted)]\n",
    "    valid_predicted = predicted[~np.isnan(actual) & ~np.isnan(predicted)]\n",
    "    if len(valid_actual) > 0:\n",
    "        min_val = min(np.min(valid_actual), np.min(valid_predicted))\n",
    "        max_val = max(np.max(valid_actual), np.max(valid_predicted))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "\n",
    "    plt.title('Predicted vs Actual 4KB Memory Usage', fontsize=14)\n",
    "    plt.xlabel('Actual 4KB Memory (MB)', fontsize=12)\n",
    "    plt.ylabel('Predicted 4KB Memory (MB)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Add correlation annotation\n",
    "    plt.annotate(f\"Correlation: {corr:.4f}\", xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                 fontsize=12)\n",
    "\n",
    "    # 4. Relative error histogram\n",
    "    plt.subplot(3, 2, 5)\n",
    "    rel_error = (error / (actual + 1e-10)) * 100  # Percentage error\n",
    "    valid_rel_error = rel_error[~np.isnan(rel_error)]\n",
    "    if len(valid_rel_error) > 0:\n",
    "        plt.hist(valid_rel_error, bins=20, alpha=0.7, color='green')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No valid data for histogram', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.title('Distribution of Percentage Error (4KB)', fontsize=14)\n",
    "    plt.xlabel('Percentage Error (%)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Cumulative error\n",
    "    plt.subplot(3, 2, 6)\n",
    "    cum_error = np.nancumsum(error)\n",
    "    plt.plot(x_values, cum_error, 'g-', linewidth=2)\n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "    plt.title('Cumulative Error Over Time (4KB)', fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel('Cumulative Error (MB)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add overall title and adjust layout\n",
    "    plt.suptitle('4KB Memory Prediction Analysis with 2MB RSS', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # Save the figure\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'correlation': corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_analysis(model, X_test, y_test, feature_names, target_scaler):\n",
    "    \"\"\"\n",
    "    Analyze feature importance by permutation and feature ablation.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        X_test: Test features.\n",
    "        y_test: Test targets.\n",
    "        feature_names: List of feature names.\n",
    "        target_scaler: Scaler for the target.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with feature importance scores.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing feature importance...\")\n",
    "\n",
    "    # Get baseline performance\n",
    "    baseline_pred = model.predict(X_test)\n",
    "    if np.all(np.isnan(baseline_pred)):\n",
    "        print(\"Warning: All baseline predictions are NaN. Skipping feature importance analysis.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    baseline_mse = np.mean(np.square(\n",
    "        target_scaler.inverse_transform(y_test.reshape(-1, 1)) -\n",
    "        target_scaler.inverse_transform(baseline_pred)\n",
    "    ))\n",
    "    print(f\"Baseline MSE: {baseline_mse:.4f}\")\n",
    "\n",
    "    # Permutation importance\n",
    "    importances = []\n",
    "    for i in range(X_test.shape[2]):\n",
    "        # Permute one feature at a time\n",
    "        X_permuted = X_test.copy()\n",
    "        X_permuted[:, :, i] = np.random.permutation(X_permuted[:, :, i])\n",
    "\n",
    "        # Get predictions with permuted feature\n",
    "        permuted_pred = model.predict(X_permuted)\n",
    "\n",
    "        # Calculate MSE with permuted feature\n",
    "        permuted_mse = np.mean(np.square(\n",
    "            target_scaler.inverse_transform(y_test.reshape(-1, 1)) -\n",
    "            target_scaler.inverse_transform(permuted_pred)\n",
    "        ))\n",
    "\n",
    "        # Calculate importance as increase in error\n",
    "        importance = permuted_mse - baseline_mse\n",
    "        relative_importance = (permuted_mse / baseline_mse) - 1  # % increase in error\n",
    "\n",
    "        importances.append({\n",
    "            'feature': feature_names[i],\n",
    "            'importance': importance,\n",
    "            'relative_importance': relative_importance * 100,  # as percentage\n",
    "            'permuted_mse': permuted_mse\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and sort\n",
    "    importance_df = pd.DataFrame(importances).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['feature'], importance_df['relative_importance'])\n",
    "    plt.title('Feature Importance (% Increase in Error When Feature is Permuted)')\n",
    "    plt.xlabel('% Increase in MSE')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/feature_importance.png')\n",
    "    plt.close()\n",
    "    print(\"Feature importance plot saved to output/feature_importance.png\")\n",
    "\n",
    "    # Feature correlation analysis\n",
    "    original_predictions = baseline_pred.flatten()\n",
    "\n",
    "    # For each pair of features, analyze correlation in prediction impact\n",
    "    feature_impact_corr = np.zeros((len(feature_names), len(feature_names)))\n",
    "\n",
    "    for i in range(X_test.shape[2]):\n",
    "        X_i = X_test.copy()\n",
    "        X_i[:, :, i] = np.random.permutation(X_i[:, :, i])\n",
    "        pred_i = model.predict(X_i).flatten()\n",
    "        impact_i = pred_i - original_predictions\n",
    "\n",
    "        for j in range(i+1, X_test.shape[2]):\n",
    "            X_j = X_test.copy()\n",
    "            X_j[:, :, j] = np.random.permutation(X_j[:, :, j])\n",
    "            pred_j = model.predict(X_j).flatten()\n",
    "            impact_j = pred_j - original_predictions\n",
    "\n",
    "            # Correlation between impact of permuting feature i and feature j\n",
    "            correlation = np.corrcoef(impact_i, impact_j)[0, 1]\n",
    "            feature_impact_corr[i, j] = correlation\n",
    "            feature_impact_corr[j, i] = correlation\n",
    "\n",
    "    # Set diagonal to 1.0\n",
    "    np.fill_diagonal(feature_impact_corr, 1.0)\n",
    "\n",
    "    # Visualize feature impact correlations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(feature_impact_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.title('Feature Impact Correlation Matrix')\n",
    "    plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "    plt.yticks(range(len(feature_names)), feature_names)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/feature_impact_correlation.png')\n",
    "    plt.close()\n",
    "    print(\"Feature impact correlation plot saved to feature_impact_correlation.png\")\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CROSS-PAGE MEMORY PREDICTION MODEL (2MB to 4KB)\n",
      "==================================================\n",
      "\n",
      "=== Loading 2MB Page Data ===\n",
      "Loading 2MB RSS data from: /home/anish/KernMLOps/data/curated/gap/2d801a0a-e668-4398-b7c3-6332c90607be/mm_rss_stat.end.parquet\n",
      "2MB RSS data shape: (1341019, 6)\n",
      "2MB RSS columns: ['pid', 'tgid', 'ts_ns', 'member', 'count', 'collection_id']\n",
      "Converted column member from object to numeric\n",
      "Converted column collection_id from object to numeric\n",
      "Loading 2MB DTLB data from: /home/anish/KernMLOps/data/curated/gap/2d801a0a-e668-4398-b7c3-6332c90607be/dtlb_misses.end.parquet\n",
      "2MB DTLB data shape: (933014, 8)\n",
      "2MB DTLB columns: ['cpu', 'pid', 'tgid', 'ts_uptime_us', 'cumulative_dtlb_misses', 'pmu_enabled_time_us', 'pmu_running_time_us', 'collection_id']\n",
      "Converted column collection_id from object to numeric\n",
      "Loading 2MB ITLB data from: /home/anish/KernMLOps/data/curated/gap/2d801a0a-e668-4398-b7c3-6332c90607be/itlb_misses.end.parquet\n",
      "2MB ITLB data shape: (7507, 8)\n",
      "2MB ITLB columns: ['cpu', 'pid', 'tgid', 'ts_uptime_us', 'cumulative_itlb_misses', 'pmu_enabled_time_us', 'pmu_running_time_us', 'collection_id']\n",
      "Converted column collection_id from object to numeric\n",
      "Converted timestamp from ns to sec\n",
      "Normalized timestamps: min=245314.010875096, range=[0, 59.79291449001175]\n",
      "Converted 2MB DTLB timestamp from us to sec\n",
      "Normalized timestamps: min=245310.209942, range=[0, 63.580543000018224]\n",
      "Found 1 2MB DTLB columns with variance\n",
      "Selected 2MB DTLB column: cumulative_dtlb_misses\n",
      "Converted 2MB ITLB timestamp from us to sec\n",
      "Normalized timestamps: min=245310.225643, range=[0, 63.52038099998026]\n",
      "Found 1 2MB ITLB columns with variance\n",
      "Selected 2MB ITLB column: cumulative_itlb_misses\n",
      "\n",
      "=== Loading 4KB Page Data ===\n",
      "Loading 4KB RSS data from: /home/anish/KernMLOps/data/curated/gap/57f484a8-a7a4-48a3-969b-ba86f524065a/mm_rss_stat.end.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1211236/2693419029.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_1211236/2693419029.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4KB RSS data shape: (1511096, 6)\n",
      "4KB RSS columns: ['pid', 'tgid', 'ts_ns', 'member', 'count', 'collection_id']\n",
      "Converted column member from object to numeric\n",
      "Converted column collection_id from object to numeric\n",
      "Loading 4KB DTLB data from: /home/anish/KernMLOps/data/curated/gap/57f484a8-a7a4-48a3-969b-ba86f524065a/dtlb_misses.end.parquet\n",
      "4KB DTLB data shape: (830993, 8)\n",
      "4KB DTLB columns: ['cpu', 'pid', 'tgid', 'ts_uptime_us', 'cumulative_dtlb_misses', 'pmu_enabled_time_us', 'pmu_running_time_us', 'collection_id']\n",
      "Converted column collection_id from object to numeric\n",
      "Loading 4KB ITLB data from: /home/anish/KernMLOps/data/curated/gap/57f484a8-a7a4-48a3-969b-ba86f524065a/itlb_misses.end.parquet\n",
      "4KB ITLB data shape: (8888, 8)\n",
      "4KB ITLB columns: ['cpu', 'pid', 'tgid', 'ts_uptime_us', 'cumulative_itlb_misses', 'pmu_enabled_time_us', 'pmu_running_time_us', 'collection_id']\n",
      "Converted column collection_id from object to numeric\n",
      "Converted timestamp from ns to sec\n",
      "Normalized timestamps: min=245676.387361545, range=[0, 59.736539253004594]\n",
      "Converted 4KB DTLB timestamp from us to sec\n",
      "Normalized timestamps: min=245672.520222, range=[0, 63.54820499999914]\n",
      "Found 1 4KB DTLB columns with variance\n",
      "Selected 4KB DTLB column: cumulative_dtlb_misses\n",
      "Converted 4KB ITLB timestamp from us to sec\n",
      "Normalized timestamps: min=245672.530019, range=[0, 63.5373430000036]\n",
      "Found 1 4KB ITLB columns with variance\n",
      "Selected 4KB ITLB column: cumulative_itlb_misses\n",
      "\n",
      "=== Creating Integrated Dataset ===\n",
      "Creating memory-change aligned dataset...\n",
      "2MB RSS time range: 0.0 to 59.79291449001175\n",
      "4KB RSS time range: 0.0 to 59.736539253004594\n",
      "2MB DTLB time range: 0.0 to 63.580543000018224\n",
      "2MB ITLB time range: 0.0 to 63.52038099998026\n",
      "4KB DTLB time range: 0.0 to 63.54820499999914\n",
      "4KB ITLB time range: 0.0 to 63.5373430000036\n",
      "Checking for duplicate timestamps...\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1211236/2693419029.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_1211236/2693419029.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 63 duplicates\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n",
      "  Removed 23473 duplicates\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n",
      "  Removed 35 duplicates\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n",
      "  Removed 61 duplicates\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n",
      "  Removed 18669 duplicates\n",
      "  Found duplicate timestamps in time_sec_normalized; removing duplicates...\n",
      "  Removed 10 duplicates\n",
      "Identifying memory change points...\n",
      "Found 0 memory change points in 2MB data\n",
      "Found 0 memory change points in 4KB data\n",
      "Aligning memory changes with TLB measurements using merge_asof...\n",
      "Created 2MB integrated dataset with 0 rows and 0 columns\n",
      "Created 4KB integrated dataset with 0 rows and 0 columns\n",
      "2MB memory metrics available: []\n",
      "4KB memory metrics available: []\n",
      "\n",
      "=== Preparing Cross-Configuration Data with target anon_mb ===\n",
      "Error in main function: Both 2MB and 4KB integrated data are required and must not be empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1211236/1745304503.py\", line 68, in main\n",
      "    cross_config_df = prepare_cross_config_data(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1211236/3606396847.py\", line 21, in prepare_cross_config_data\n",
      "    raise ValueError(\"Both 2MB and 4KB integrated data are required and must not be empty\")\n",
      "ValueError: Both 2MB and 4KB integrated data are required and must not be empty\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the cross-page prediction model.\"\"\"\n",
    "    try:\n",
    "\n",
    "        # Add this at the beginning of your script\n",
    "        base_dir = os.path.expanduser(\"~/KernMLOps\")\n",
    "\n",
    "        kb2_id = \"2d801a0a-e668-4398-b7c3-6332c90607be\"\n",
    "        # Then update your paths\n",
    "        kb2_paths = {\n",
    "            'rss': os.path.join(base_dir, f\"data/curated/gap/{kb2_id}/mm_rss_stat.end.parquet\"),\n",
    "            'dtlb': os.path.join(base_dir, f\"data/curated/gap/{kb2_id}/dtlb_misses.end.parquet\"),\n",
    "            'itlb': os.path.join(base_dir, f\"data/curated/gap/{kb2_id}/itlb_misses.end.parquet\")\n",
    "        }\n",
    "\n",
    "        kb4_id = \"57f484a8-a7a4-48a3-969b-ba86f524065a\"\n",
    "        kb4_paths = {\n",
    "            'rss': os.path.join(base_dir, f\"data/curated/gap/{kb4_id}/mm_rss_stat.end.parquet\"),\n",
    "            'dtlb': os.path.join(base_dir, f\"data/curated/gap/{kb4_id}/dtlb_misses.end.parquet\"),\n",
    "            'itlb': os.path.join(base_dir, f\"data/curated/gap/{kb4_id}/itlb_misses.end.parquet\")\n",
    "        }\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"CROSS-PAGE MEMORY PREDICTION MODEL (2MB to 4KB)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\n=== Loading 2MB Page Data ===\")\n",
    "        kb2_data = {}\n",
    "        kb2_data['rss'] = load_parquet_data(kb2_paths['rss'], \"2MB RSS\")\n",
    "        kb2_data['dtlb'] = load_parquet_data(kb2_paths['dtlb'], \"2MB DTLB\")\n",
    "        kb2_data['itlb'] = load_parquet_data(kb2_paths['itlb'], \"2MB ITLB\")\n",
    "        \n",
    "        if kb2_data['rss'] is not None:\n",
    "            kb2_data['rss'], kb2_rss_cols = process_rss_data(kb2_data['rss'], page_size_mb=0.004)\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load 2MB RSS data\")\n",
    "        if kb2_data['dtlb'] is not None:\n",
    "            kb2_data['dtlb'], kb2_dtlb_cols = process_tlb_data(kb2_data['dtlb'], \"2MB DTLB\")\n",
    "        if kb2_data['itlb'] is not None:\n",
    "            kb2_data['itlb'], kb2_itlb_cols = process_tlb_data(kb2_data['itlb'], \"2MB ITLB\")\n",
    "        \n",
    "        print(\"\\n=== Loading 4KB Page Data ===\")\n",
    "        kb4_data = {}\n",
    "        kb4_data['rss'] = load_parquet_data(kb4_paths['rss'], \"4KB RSS\")\n",
    "        kb4_data['dtlb'] = load_parquet_data(kb4_paths['dtlb'], \"4KB DTLB\")\n",
    "        kb4_data['itlb'] = load_parquet_data(kb4_paths['itlb'], \"4KB ITLB\")\n",
    "        \n",
    "        if kb4_data['rss'] is not None:\n",
    "            kb4_data['rss'], kb4_rss_cols = process_rss_data(kb4_data['rss'], page_size_mb=0.004)\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load 4KB RSS data\")\n",
    "        if kb4_data['dtlb'] is not None:\n",
    "            kb4_data['dtlb'], kb4_dtlb_cols = process_tlb_data(kb4_data['dtlb'], \"4KB DTLB\")\n",
    "        if kb4_data['itlb'] is not None:\n",
    "            kb4_data['itlb'], kb4_itlb_cols = process_tlb_data(kb4_data['itlb'], \"4KB ITLB\")\n",
    "        \n",
    "        print(\"\\n=== Creating Integrated Dataset ===\")\n",
    "        integrated_data = create_memory_change_dataset(kb2_data, kb4_data)\n",
    "        \n",
    "        # Set desired target memory metric (default is usually \"anon_mb\")\n",
    "        target_metric = \"anon_mb\"\n",
    "        \n",
    "        print(f\"\\n=== Preparing Cross-Configuration Data with target {target_metric} ===\")\n",
    "        # Use sliding window instead of binning\n",
    "        cross_config_df = prepare_cross_config_data(\n",
    "            integrated_data, \n",
    "            target_metric, \n",
    "            window_size=1.0,  # 1-second window\n",
    "            step_size=0.5     # 0.5-second step\n",
    "        )\n",
    "        \n",
    "        # Save the prepared dataset\n",
    "        cross_config_df.to_csv('output/cross_config_dataset.csv', index=False)\n",
    "        print(\"Prepared dataset saved to output/cross_config_dataset.csv\")\n",
    "        \n",
    "        # Define target and feature columns\n",
    "        target_column = '4kb_target'\n",
    "        feature_columns = [col for col in cross_config_df.columns \n",
    "                          if col not in ['window_start', 'window_end', 'time_sec', target_column, '2mb_rss']]\n",
    "        \n",
    "        print(\"\\n=== Preparing LSTM Sequences ===\")\n",
    "        seq_length = 5  # Adjust based on temporal dynamics\n",
    "        X, y, scalers, valid_features = prepare_lstm_sequences(\n",
    "            cross_config_df, target_column, feature_columns, seq_length=seq_length\n",
    "        )\n",
    "        if len(X) < 10:\n",
    "            print(f\"Not enough data for training. Only {len(X)} sequences created.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        print(\"\\n=== Splitting Data into Train/Test Sets ===\")\n",
    "        # Use time-based split (not random) to preserve temporal structure\n",
    "        test_size = 0.2\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "        \n",
    "        print(\"\\n=== Cross Validation ===\")\n",
    "        # Perform cross-validation\n",
    "        cv_metrics, best_model = cross_validate_model(\n",
    "            X_train, y_train, \n",
    "            input_shape=(X.shape[1], X.shape[2]),\n",
    "            n_splits=min(5, len(X_train) // 10),  # Adjust based on data size\n",
    "            epochs=50,\n",
    "            batch_size=min(32, len(X_train) // 5)\n",
    "        )\n",
    "        \n",
    "        # Save cross-validation results\n",
    "        pd.DataFrame(cv_metrics).to_csv('output/cross_validation_results.csv', index=False)\n",
    "        print(\"Cross-validation results saved to output/cross_validation_results.csv\")\n",
    "        \n",
    "        # Save the best model with proper extension\n",
    "        best_model.save('output/cross_page_prediction_model.keras')\n",
    "        print(\"Best model saved to output/cross_page_prediction_model.keras\")\n",
    "        \n",
    "        # Alternatively, save as H5 format\n",
    "        best_model.save('output/cross_page_prediction_model.h5')\n",
    "        print(\"Best model also saved in H5 format to output/cross_page_prediction_model.h5\")\n",
    "        \n",
    "        print(\"\\n=== Model Evaluation ===\")\n",
    "        # Generate predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_test_orig = scalers[target_column].inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "        y_pred_orig = scalers[target_column].inverse_transform(y_pred).flatten()\n",
    "        \n",
    "        # Check for NaN predictions\n",
    "        if np.all(np.isnan(y_test_orig)) or np.all(np.isnan(y_pred_orig)):\n",
    "            print(\"Error: All test predictions or actual values are NaN. Skipping evaluation.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Get 2MB RSS values for test set\n",
    "        start_idx = len(X_train) + seq_length\n",
    "        end_idx = start_idx + len(y_test)\n",
    "        kb2_rss_test = cross_config_df['2mb_rss'].values[start_idx:end_idx]\n",
    "        \n",
    "        # Get time steps if available\n",
    "        time_values = None\n",
    "        if 'time_sec' in cross_config_df.columns:\n",
    "            time_values = cross_config_df['time_sec'].values[start_idx:end_idx]\n",
    "        \n",
    "        # Visualize predictions\n",
    "        metrics = visualize_predictions(\n",
    "            actual_values=y_test_orig,\n",
    "            predicted_values=y_pred_orig,\n",
    "            kb2_rss_values=kb2_rss_test,\n",
    "            time_steps=time_values,\n",
    "            save_path='output/test_prediction_comparison.png'\n",
    "        )\n",
    "        \n",
    "        # Save metrics\n",
    "        with open('output/evaluation_metrics.txt', 'w') as f:\n",
    "            for k, v in metrics.items():\n",
    "                f.write(f\"{k}: {v}\\n\")\n",
    "        print(\"Evaluation metrics saved to output/evaluation_metrics.txt\")\n",
    "        \n",
    "        print(\"\\n=== Feature Importance Analysis ===\")\n",
    "        # Analyze feature importance\n",
    "        importance_df = feature_importance_analysis(\n",
    "            best_model, X_test, y_test, \n",
    "            valid_features, scalers[target_column]\n",
    "        )\n",
    "        \n",
    "        # Save feature importance\n",
    "        importance_df.to_csv('output/feature_importance.csv', index=False)\n",
    "        print(\"Feature importance results saved to output/feature_importance.csv\")\n",
    "        \n",
    "        print(\"\\n=== Generating Full Prediction Comparison ===\")\n",
    "        all_X = np.vstack([X_train, X_test])\n",
    "        all_y = np.vstack([y_train, y_test])\n",
    "        \n",
    "        # Get predictions for all data\n",
    "        all_pred = best_model.predict(all_X)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        all_y_orig = scalers[target_column].inverse_transform(all_y)\n",
    "        all_pred_orig = scalers[target_column].inverse_transform(all_pred)\n",
    "        \n",
    "        # Get 2MB RSS values for full dataset\n",
    "        kb2_rss_full = cross_config_df['2mb_rss'].values[seq_length:seq_length+len(all_y_orig)]\n",
    "        \n",
    "        # Create time array\n",
    "        if 'time_sec' in cross_config_df.columns:\n",
    "            time_values = cross_config_df['time_sec'].values[seq_length:seq_length+len(all_y_orig)]\n",
    "        else:\n",
    "            time_values = None\n",
    "        \n",
    "        # Visualize full comparison with enhanced details\n",
    "        metrics_full = visualize_predictions(\n",
    "            actual_values=all_y_orig,\n",
    "            predicted_values=all_pred_orig,\n",
    "            kb2_rss_values=kb2_rss_full,\n",
    "            time_steps=time_values,\n",
    "            save_path='output/full_prediction_comparison.png'\n",
    "        )\n",
    "        print(\"Full prediction comparison saved to output/full_prediction_comparison.png\")\n",
    "        \n",
    "        print(\"\\n=== Analysis Complete ===\")\n",
    "        print(f\"Model achieved RMSE of {metrics['rmse']:.4f} ({metrics['mape']:.2f}% MAPE)\")\n",
    "        print(f\"Top 3 important features: {', '.join(importance_df['feature'].head(3).tolist())}\")\n",
    "        \n",
    "        return best_model, metrics, importance_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics, importance = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
