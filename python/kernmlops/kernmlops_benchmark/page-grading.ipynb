{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 07:20:12.952857: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 07:20:12.956118: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-15 07:20:12.966624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744701612.983917 1093926 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744701612.989080 1093926 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744701613.002414 1093926 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744701613.002434 1093926 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744701613.002436 1093926 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744701613.002437 1093926 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 07:20:13.008861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-15 07:20:17.516245: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure CPU fallback if GPU is unavailable\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_data(file_path, source_name):\n",
    "    \"\"\"Load a parquet file and return a DataFrame\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: {source_name} file not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading {source_name} data from: {file_path}\")\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"{source_name} data shape: {df.shape}\")\n",
    "    print(f\"{source_name} columns: {df.columns.tolist()}\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                print(f\"Converted column {col} from object to numeric\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert column {col} to numeric: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(df, time_col='time_sec'):\n",
    "    \"\"\"Normalize timestamps to start from zero.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    min_time = df[time_col].min()\n",
    "    if pd.isna(min_time):\n",
    "        print(f\"Warning: No valid timestamps in {time_col}\")\n",
    "        return df\n",
    "    df[f'{time_col}_normalized'] = df[time_col] - min_time\n",
    "    print(f\"Normalized timestamps: min={min_time}, range=[0, {df[time_col].max() - min_time}]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rss_data(rss_df, page_size_mb):\n",
    "    \"\"\"Process RSS data with appropriate page size conversion\"\"\"\n",
    "    if rss_df is None or rss_df.empty:\n",
    "        print(\"Warning: RSS DataFrame is empty\")\n",
    "        return None, []\n",
    "\n",
    "    if 'ts_ns' in rss_df.columns:\n",
    "        rss_df['time_sec'] = rss_df['ts_ns'] / 1e9\n",
    "        print(\"Converted timestamp from ns to sec\")\n",
    "\n",
    "    rss_df = rss_df.sort_values('time_sec')\n",
    "    rss_df = normalize_timestamps(rss_df)\n",
    "    rss_df['page_size_mb'] = page_size_mb\n",
    "\n",
    "    rss_cols = []\n",
    "    for col in ['anon', 'file', 'swap', 'shmem']:\n",
    "        if col in rss_df.columns:\n",
    "            if rss_df[col].std() > 0:\n",
    "                rss_cols.append(col)\n",
    "                print(f\"Found RSS column with variance: {col}\")\n",
    "\n",
    "    for col in rss_cols:\n",
    "        new_col = f\"{col}_mb\"\n",
    "        rss_df[new_col] = rss_df[col] * page_size_mb\n",
    "        print(f\"Converted {col} to MB using {page_size_mb}MB page size\")\n",
    "\n",
    "    # Ensure file_mb exists\n",
    "    if 'file' not in rss_cols:\n",
    "        print(\"Warning: 'file' column missing or has zero variance, adding file_mb with zeros\")\n",
    "        rss_df['file_mb'] = 0\n",
    "        rss_cols.append('file')\n",
    "\n",
    "    print(f\"Processed RSS columns: {rss_df.columns.tolist()}\")\n",
    "    return rss_df, rss_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tlb_data(tlb_df, tlb_type):\n",
    "    \"\"\"Process TLB miss data\"\"\"\n",
    "    if tlb_df is None or tlb_df.empty:\n",
    "        print(f\"Warning: {tlb_type} DataFrame is empty\")\n",
    "        return None, []\n",
    "\n",
    "    if 'ts_uptime_us' in tlb_df.columns:\n",
    "        tlb_df['time_sec'] = tlb_df['ts_uptime_us'] / 1e6\n",
    "        print(f\"Converted {tlb_type} timestamp from us to sec\")\n",
    "\n",
    "    tlb_df = tlb_df.sort_values('time_sec')\n",
    "    tlb_df = normalize_timestamps(tlb_df)\n",
    "\n",
    "    tlb_cols = []\n",
    "    for col in tlb_df.columns:\n",
    "        if 'miss' in col.lower():\n",
    "            if tlb_df[col].std() > 0:\n",
    "                tlb_cols.append(col)\n",
    "    print(f\"Found {len(tlb_cols)} {tlb_type} columns with variance: {tlb_cols}\")\n",
    "\n",
    "    main_tlb_col = None\n",
    "    for col in tlb_cols:\n",
    "        if 'cumulative' in col.lower() or 'total' in col.lower():\n",
    "            main_tlb_col = col\n",
    "            break\n",
    "\n",
    "    if main_tlb_col is None and tlb_cols:\n",
    "        main_tlb_col = tlb_cols[0]\n",
    "\n",
    "    if main_tlb_col:\n",
    "        print(f\"Selected {tlb_type} column: {main_tlb_col}\")\n",
    "        tlb_df[f'{tlb_type}_misses'] = tlb_df[main_tlb_col]\n",
    "        tlb_df[f'{tlb_type}_misses_rate'] = tlb_df[f'{tlb_type}_misses'].diff() / tlb_df['time_sec_normalized'].diff()\n",
    "        tlb_df[f'{tlb_type}_misses_rate'].fillna(0, inplace=True)\n",
    "        print(f\"Generated TLB columns: {[f'{tlb_type}_misses', f'{tlb_type}_misses_rate']}\")\n",
    "        return tlb_df, [f'{tlb_type}_misses', f'{tlb_type}_misses_rate']\n",
    "    else:\n",
    "        print(f\"Warning: No suitable {tlb_type} column found\")\n",
    "        return tlb_df, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_memory_change_dataset(kb2_data, kb4_data):\n",
    "    \"\"\"Create a dataset aligning memory changes with TLB measurements\"\"\"\n",
    "    print(\"Creating memory-change aligned dataset...\")\n",
    "    kb2_rss = kb2_data.get('rss')\n",
    "    kb2_dtlb = kb2_data.get('dtlb')\n",
    "    kb2_itlb = kb2_data.get('itlb')\n",
    "    kb4_rss = kb4_data.get('rss')\n",
    "    kb4_dtlb = kb4_data.get('dtlb')\n",
    "    kb4_itlb = kb4_data.get('itlb')\n",
    "\n",
    "    if kb2_rss is None or kb4_rss is None:\n",
    "        raise ValueError(\"Both 2MB and 4KB RSS data are required\")\n",
    "\n",
    "    def deduplicate_timestamps(df, time_col='time_sec_normalized'):\n",
    "        if df is None:\n",
    "            return None\n",
    "        if df[time_col].duplicated().any():\n",
    "            print(f\"Found duplicate timestamps in {time_col}; removing duplicates...\")\n",
    "            df = df.sort_values(time_col).drop_duplicates(subset=[time_col], keep='last')\n",
    "        return df\n",
    "\n",
    "    for name, df in [('2MB RSS', kb2_rss), ('2MB DTLB', kb2_dtlb), ('2MB ITLB', kb2_itlb),\n",
    "                     ('4KB RSS', kb4_rss), ('4KB DTLB', kb4_dtlb), ('4KB ITLB', kb4_itlb)]:\n",
    "        if df is not None:\n",
    "            df = deduplicate_timestamps(df)\n",
    "            if name == '2MB RSS': kb2_rss = df\n",
    "            elif name == '2MB DTLB': kb2_dtlb = df\n",
    "            elif name == '2MB ITLB': kb2_itlb = df\n",
    "            elif name == '4KB RSS': kb4_rss = df\n",
    "            elif name == '4KB DTLB': kb4_dtlb = df\n",
    "            elif name == '4KB ITLB': kb4_itlb = df\n",
    "\n",
    "    def find_memory_changes(df, memory_cols, threshold=0.001):\n",
    "        if df is None:\n",
    "            return pd.DataFrame()\n",
    "        change_points = []\n",
    "        for col in memory_cols:\n",
    "            col_mb = f\"{col}_mb\"\n",
    "            if col_mb in df.columns:\n",
    "                diff = df[col_mb].diff().abs()\n",
    "                changes = df[diff > threshold].copy()\n",
    "                if not changes.empty:\n",
    "                    changes['memory_metric'] = col_mb\n",
    "                    changes['memory_value'] = changes[col_mb]\n",
    "                    change_points.append(changes[['time_sec_normalized', 'memory_metric', 'memory_value']])\n",
    "        if not change_points:\n",
    "            return pd.DataFrame()\n",
    "        return pd.concat(change_points).sort_values('time_sec_normalized')\n",
    "\n",
    "    kb2_memory_candidates = [col for col in ['anon', 'file'] if col in kb2_rss.columns]\n",
    "    kb4_memory_candidates = [col for col in ['anon', 'file'] if col in kb4_rss.columns]\n",
    "\n",
    "    kb2_changes = find_memory_changes(kb2_rss, kb2_memory_candidates)\n",
    "    kb4_changes = find_memory_changes(kb4_rss, kb4_memory_candidates)\n",
    "\n",
    "    print(f\"Found {len(kb2_changes)} memory change points in 2MB data\")\n",
    "    print(f\"Found {len(kb4_changes)} memory change points in 4KB data\")\n",
    "\n",
    "    def align_with_tlb(changes_df, dtlb_df, itlb_df):\n",
    "        if changes_df is None or changes_df.empty:\n",
    "            return changes_df\n",
    "        result = changes_df.copy()\n",
    "        if dtlb_df is not None:\n",
    "            dtlb_df = dtlb_df.sort_values('time_sec_normalized')\n",
    "            dtlb_columns = [col for col in dtlb_df.columns\n",
    "                           if col in ['2MB_DTLB_misses', '2MB_DTLB_misses_rate', '4KB_DTLB_misses', '4KB_DTLB_misses_rate']]\n",
    "            if dtlb_columns:\n",
    "                result = pd.merge_asof(\n",
    "                    result,\n",
    "                    dtlb_df[['time_sec_normalized'] + dtlb_columns],\n",
    "                    on='time_sec_normalized',\n",
    "                    direction='backward',\n",
    "                    suffixes=(\"\", \"_dtlb\")\n",
    "                )\n",
    "                print(f\"DTLB merged columns: {dtlb_columns}\")\n",
    "            else:\n",
    "                print(\"Warning: No valid DTLB columns to merge\")\n",
    "        if itlb_df is not None:\n",
    "            itlb_df = itlb_df.sort_values('time_sec_normalized')\n",
    "            itlb_columns = [col for col in itlb_df.columns\n",
    "                           if col in ['2MB_ITLB_misses', '2MB_ITLB_misses_rate', '4KB_ITLB_misses', '4KB_ITLB_misses_rate']]\n",
    "            if itlb_columns:\n",
    "                result = pd.merge_asof(\n",
    "                    result,\n",
    "                    itlb_df[['time_sec_normalized'] + itlb_columns],\n",
    "                    on='time_sec_normalized',\n",
    "                    direction='backward',\n",
    "                    suffixes=(\"\", \"_itlb\")\n",
    "                )\n",
    "                print(f\"ITLB merged columns: {itlb_columns}\")\n",
    "            else:\n",
    "                print(\"Warning: No valid ITLB columns to merge\")\n",
    "        print(f\"Aligned DataFrame columns: {result.columns.tolist()}\")\n",
    "        return result\n",
    "\n",
    "    kb2_df = align_with_tlb(kb2_changes, kb2_dtlb, kb2_itlb)\n",
    "    kb4_df = align_with_tlb(kb4_changes, kb4_dtlb, kb4_itlb)\n",
    "\n",
    "    return {\n",
    "        'kb2_df': kb2_df,\n",
    "        'kb4_df': kb4_df,\n",
    "        'kb2_memory_metrics': kb2_df['memory_metric'].unique().tolist() if not kb2_df.empty else [],\n",
    "        'kb4_memory_metrics': kb4_df['memory_metric'].unique().tolist() if not kb4_df.empty else []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cross_config_data(integrated_data, target_metric='anon_mb', window_size=10.0, step_size=2.0):\n",
    "    \"\"\"Prepare data for model evaluation\"\"\"\n",
    "    kb2_df = integrated_data.get('kb2_df')\n",
    "    kb4_df = integrated_data.get('kb4_df')\n",
    "    \n",
    "    if kb2_df is None or kb2_df.empty or kb4_df is None or kb4_df.empty:\n",
    "        raise ValueError(\"Both 2MB and 4KB integrated data are required\")\n",
    "\n",
    "    print(f\"kb2_df columns: {kb2_df.columns.tolist()}\")\n",
    "    print(f\"kb4_df columns: {kb4_df.columns.tolist()}\")\n",
    "    print(f\"kb4_df memory_metric values: {kb4_df['memory_metric'].unique().tolist()}\")\n",
    "    available_metrics = kb4_df['memory_metric'].unique()\n",
    "    if target_metric not in available_metrics:\n",
    "        print(f\"Warning: Target metric {target_metric} not found. Available: {available_metrics}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame to prevent further errors\n",
    "\n",
    "    kb4_targets = kb4_df[kb4_df['memory_metric'] == target_metric][['time_sec_normalized', 'memory_value']].copy()\n",
    "    kb4_targets = kb4_targets.rename(columns={'memory_value': '4kb_target'})\n",
    "    print(f\"kb4_targets shape: {kb4_targets.shape}\")\n",
    "    if kb4_targets.empty:\n",
    "        print(\"Error: No data found for target_metric in kb4_df\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    kb2_rss = kb2_df[kb2_df['memory_metric'] == target_metric][['time_sec_normalized', 'memory_value']].copy()\n",
    "    kb2_rss = kb2_rss.rename(columns={'memory_value': '2mb_rss'})\n",
    "    print(f\"kb2_rss shape: {kb2_rss.shape}\")\n",
    "\n",
    "    kb4_targets = kb4_targets.sort_values('time_sec_normalized')\n",
    "    kb2_df = kb2_df.sort_values('time_sec_normalized')\n",
    "    kb2_rss = kb2_rss.sort_values('time_sec_normalized')\n",
    "\n",
    "    min_time = min(kb4_targets['time_sec_normalized'].min(), kb2_df['time_sec_normalized'].min())\n",
    "    max_time = max(kb4_targets['time_sec_normalized'].max(), kb2_df['time_sec_normalized'].max())\n",
    "    window_starts = np.arange(min_time, max_time - window_size + step_size, step_size)\n",
    "    print(f\"Window range: [{min_time}, {max_time}], {len(window_starts)} windows\")\n",
    "\n",
    "    window_data = []\n",
    "    expected_features = ['memory_value', 'file_mb', '2MB_DTLB_misses', '2MB_DTLB_misses_rate',\n",
    "                        '2MB_ITLB_misses', '2MB_ITLB_misses_rate']\n",
    "    kb2_features = [col for col in expected_features if col in kb2_df.columns]\n",
    "    print(f\"Available kb2_features: {kb2_features}\")\n",
    "    \n",
    "    if len(kb2_features) < 6:\n",
    "        print(f\"Warning: Only {len(kb2_features)} features available, padding with zeros\")\n",
    "        for i in range(6 - len(kb2_features)):\n",
    "            kb2_df[f'dummy_feature_{i}'] = 0\n",
    "            kb2_features.append(f'dummy_feature_{i}')\n",
    "    \n",
    "    for start in window_starts:\n",
    "        end = start + window_size\n",
    "        kb2_window = kb2_df[(kb2_df['time_sec_normalized'] >= start) & \n",
    "                           (kb2_df['time_sec_normalized'] < end)]\n",
    "        kb4_window = kb4_targets[(kb4_targets['time_sec_normalized'] >= start) & \n",
    "                                (kb4_targets['time_sec_normalized'] < end)]\n",
    "        kb2_rss_window = kb2_rss[(kb2_rss['time_sec_normalized'] >= start) & \n",
    "                                (kb2_rss['time_sec_normalized'] < end)]\n",
    "        \n",
    "        kb4_target = kb4_window['4kb_target'].iloc[-1] if not kb4_window.empty else np.nan\n",
    "        kb2_rss_value = kb2_rss_window['2mb_rss'].iloc[-1] if not kb2_rss_window.empty else np.nan\n",
    "        kb2_agg = kb2_window[kb2_features].replace([np.inf, -np.inf], np.nan).mean().to_dict() if not kb2_window.empty else {col: np.nan for col in kb2_features}\n",
    "        \n",
    "        # Append even if kb4_target is nan to avoid empty DataFrame\n",
    "        window_data.append({\n",
    "            'window_start': start,\n",
    "            'window_end': end,\n",
    "            'time_sec': (start + end) / 2,\n",
    "            '4kb_target': kb4_target,\n",
    "            '2mb_rss': kb2_rss_value,\n",
    "            **kb2_agg\n",
    "        })\n",
    "    \n",
    "    cross_config_df = pd.DataFrame(window_data)\n",
    "    print(f\"Initial cross_config_df shape: {cross_config_df.shape}\")\n",
    "    print(f\"cross_config_df columns: {cross_config_df.columns.tolist()}\")\n",
    "    \n",
    "    if cross_config_df.empty:\n",
    "        print(\"Warning: No windows generated, returning empty DataFrame\")\n",
    "        return cross_config_df\n",
    "    \n",
    "    cross_config_df[kb2_features] = cross_config_df[kb2_features].fillna(0)\n",
    "    # Only dropna if we have data to avoid KeyError\n",
    "    if '4kb_target' in cross_config_df.columns:\n",
    "        cross_config_df = cross_config_df.dropna(subset=['4kb_target'])\n",
    "        print(f\"After dropna cross_config_df shape: {cross_config_df.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: '4kb_target' column missing in cross_config_df\")\n",
    "    \n",
    "    return cross_config_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_lstm_sequences(df, target_col, feature_cols, seq_length=5):\n",
    "    \"\"\"Prepare sequences for LSTM model\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Error: Input DataFrame is empty, cannot prepare sequences\")\n",
    "        return np.array([]), np.array([]), {}, []\n",
    "\n",
    "    expected_features = ['memory_value', 'file_mb', '2MB_DTLB_misses', '2MB_DTLB_misses_rate',\n",
    "                        '2MB_ITLB_misses', '2MB_ITLB_misses_rate']\n",
    "    valid_feature_cols = []\n",
    "    for col in expected_features:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]) and df[col].std() > 0:\n",
    "            valid_feature_cols.append(col)\n",
    "    \n",
    "    print(f\"Valid feature columns before padding: {valid_feature_cols}\")\n",
    "    if len(valid_feature_cols) < 6:\n",
    "        print(f\"Warning: Only {len(valid_feature_cols)} features with variance, padding with zeros\")\n",
    "        for i in range(6 - len(valid_feature_cols)):\n",
    "            col = f'dummy_feature_{i}'\n",
    "            df[col] = 0\n",
    "            valid_feature_cols.append(col)\n",
    "    elif len(valid_feature_cols) > 6:\n",
    "        print(f\"Warning: Found {len(valid_feature_cols)} features, trimming to 6\")\n",
    "        valid_feature_cols = valid_feature_cols[:6]\n",
    "    \n",
    "    print(f\"Final valid feature columns: {valid_feature_cols}\")\n",
    "    scalers = {}\n",
    "    scaled_data = {}\n",
    "    for col in valid_feature_cols + [target_col]:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = df[col].values.reshape(-1, 1)\n",
    "        scaled_data[col] = scaler.fit_transform(data)\n",
    "        scalers[col] = scaler\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        features_seq = [scaled_data[col][i:i+seq_length] for col in valid_feature_cols]\n",
    "        X.append(np.hstack(features_seq))\n",
    "        y.append(scaled_data[target_col][i+seq_length])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], seq_length, len(valid_feature_cols)))\n",
    "    print(f\"Generated X with shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, scalers, valid_feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(actual_values, predicted_values, kb2_rss_values, time_steps=None, save_path='output/evaluation_prediction_comparison.png'):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    actual = np.array(actual_values).flatten()\n",
    "    predicted = np.array(predicted_values).flatten()\n",
    "    kb2_rss = np.array(kb2_rss_values).flatten()\n",
    "\n",
    "    if np.all(np.isnan(actual)) or np.all(np.isnan(predicted)):\n",
    "        print(f\"Warning: Invalid data for visualization at {save_path}\")\n",
    "        return {\n",
    "            'mse': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'mae': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'correlation': np.nan\n",
    "        }\n",
    "\n",
    "    x_values = time_steps if time_steps is not None else np.arange(len(actual))\n",
    "    x_label = 'Time (seconds)' if time_steps is not None else 'Sample Index'\n",
    "\n",
    "    error = actual - predicted\n",
    "    mse = np.nanmean(np.square(error))\n",
    "    rmse = np.sqrt(mse) if not np.isnan(mse) else np.nan\n",
    "    mae = np.nanmean(np.abs(error))\n",
    "    mape = np.nanmean(np.abs(error / (actual + 1e-10))) * 100\n",
    "    r2 = 1 - (np.nansum(np.square(error)) / np.nansum(np.square(actual - np.nanmean(actual)))) if not np.all(np.isnan(actual)) else np.nan\n",
    "    corr = np.corrcoef(actual, predicted)[0, 1] if not (np.all(np.isnan(actual)) or np.all(np.isnan(predicted))) else np.nan\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(x_values, actual, 'b-', label='Actual 4KB Memory')\n",
    "    plt.plot(x_values, predicted, 'r--', label='Predicted 4KB Memory')\n",
    "    plt.plot(x_values, kb2_rss, 'g-.', label='Actual 2MB Memory')\n",
    "    plt.title('Memory Usage Comparison: 4KB (Actual vs Predicted) and 2MB', fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel('Memory Usage (MB)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    metrics_text = f\"MSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nMAPE: {mape:.2f}%\\nRÂ²: {r2:.4f}\"\n",
    "    plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                 fontsize=10)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'correlation': corr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EVALUATING CROSS-PAGE MEMORY PREDICTION MODEL\n",
      "==================================================\n",
      "\n",
      "=== Loading Model ===\n",
      "Model loaded from output/cross_page_prediction_model.keras\n",
      "\n",
      "=== Loading Data ===\n",
      "Error: 2MB RSS file not found: data/curated/mm_rss_stat/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\n",
      "Error: 2MB DTLB file not found: data/curated/dtlb_misses/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\n",
      "Error: 2MB ITLB file not found: data/curated/itlb_misses/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\n",
      "Error: 4KB RSS file not found: data/curated/mm_rss_stat/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\n",
      "Error: 4KB DTLB file not found: data/curated/dtlb_misses/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\n",
      "Error: 4KB ITLB file not found: data/curated/itlb_misses/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\n",
      "\n",
      "=== Processing Data ===\n",
      "Error during evaluation: Failed to load 2MB RSS data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1093926/2050444355.py\", line 31, in main\n",
      "    raise ValueError(\"Failed to load 2MB RSS data\")\n",
      "ValueError: Failed to load 2MB RSS data\n"
     ]
    }
   ],
   "source": [
    "def main(kb2_rss_path, kb4_rss_path, kb2_dtlb_path, kb2_itlb_path, kb4_dtlb_path, kb4_itlb_path, model_path='output/cross_page_prediction_model.keras'):\n",
    "    \"\"\"Evaluate the cross-page prediction model with RSS and TLB data\"\"\"\n",
    "    try:\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        print(\"=\"*50)\n",
    "        print(\"EVALUATING CROSS-PAGE MEMORY PREDICTION MODEL\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        print(\"\\n=== Loading Model ===\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "        print(\"\\n=== Loading Data ===\")\n",
    "        kb2_data = {\n",
    "            'rss': load_parquet_data(kb2_rss_path, \"2MB RSS\"),\n",
    "            'dtlb': load_parquet_data(kb2_dtlb_path, \"2MB DTLB\"),\n",
    "            'itlb': load_parquet_data(kb2_itlb_path, \"2MB ITLB\")\n",
    "        }\n",
    "        kb4_data = {\n",
    "            'rss': load_parquet_data(kb4_rss_path, \"4KB RSS\"),\n",
    "            'dtlb': load_parquet_data(kb4_dtlb_path, \"4KB DTLB\"),\n",
    "            'itlb': load_parquet_data(kb4_itlb_path, \"4KB ITLB\")\n",
    "        }\n",
    "\n",
    "        print(\"\\n=== Processing Data ===\")\n",
    "        if kb2_data['rss'] is not None:\n",
    "            kb2_data['rss'], kb2_rss_cols = process_rss_data(kb2_data['rss'], page_size_mb=0.004)\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load 2MB RSS data\")\n",
    "        if kb4_data['rss'] is not None:\n",
    "            kb4_data['rss'], kb4_rss_cols = process_rss_data(kb4_data['rss'], page_size_mb=0.004)\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load 4KB RSS data\")\n",
    "        \n",
    "        if kb2_data['dtlb'] is not None:\n",
    "            kb2_data['dtlb'], kb2_dtlb_cols = process_tlb_data(kb2_data['dtlb'], \"2MB_DTLB\")\n",
    "        else:\n",
    "            print(\"Warning: 2MB DTLB data missing\")\n",
    "            kb2_data['dtlb'] = None\n",
    "        if kb2_data['itlb'] is not None:\n",
    "            kb2_data['itlb'], kb2_itlb_cols = process_tlb_data(kb2_data['itlb'], \"2MB_ITLB\")\n",
    "        else:\n",
    "            print(\"Warning: 2MB ITLB data missing\")\n",
    "            kb2_data['itlb'] = None\n",
    "        \n",
    "        if kb4_data['dtlb'] is not None:\n",
    "            kb4_data['dtlb'], kb4_dtlb_cols = process_tlb_data(kb4_data['dtlb'], \"4KB_DTLB\")\n",
    "        else:\n",
    "            print(\"Warning: 4KB DTLB data missing\")\n",
    "            kb4_data['dtlb'] = None\n",
    "        if kb4_data['itlb'] is not None:\n",
    "            kb4_data['itlb'], kb4_itlb_cols = process_tlb_data(kb4_data['itlb'], \"4KB_ITLB\")\n",
    "        else:\n",
    "            print(\"Warning: 4KB ITLB data missing\")\n",
    "            kb4_data['itlb'] = None\n",
    "\n",
    "        print(\"\\n=== Creating Integrated Dataset ===\")\n",
    "        integrated_data = create_memory_change_dataset(kb2_data, kb4_data)\n",
    "\n",
    "        print(\"\\n=== Preparing Data ===\")\n",
    "        cross_config_df = prepare_cross_config_data(integrated_data, target_metric='anon_mb')\n",
    "        if cross_config_df.empty:\n",
    "            print(\"Error: Failed to generate evaluation dataset\")\n",
    "            return None, None\n",
    "        cross_config_df.to_csv('output/evaluation_dataset.csv', index=False)\n",
    "        print(\"Evaluation dataset saved to output/evaluation_dataset.csv\")\n",
    "\n",
    "        print(\"\\n=== Preparing LSTM Sequences ===\")\n",
    "        target_column = '4kb_target'\n",
    "        feature_columns = ['memory_value', 'file_mb', '2MB_DTLB_misses', '2MB_DTLB_misses_rate',\n",
    "                          '2MB_ITLB_misses', '2MB_ITLB_misses_rate']\n",
    "        X, y, scalers, valid_features = prepare_lstm_sequences(\n",
    "            cross_config_df, target_column, feature_columns, seq_length=5\n",
    "        )\n",
    "\n",
    "        if len(X) < 1:\n",
    "            print(\"Not enough data for evaluation\")\n",
    "            return None, None\n",
    "\n",
    "        print(\"\\n=== Making Predictions ===\")\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        y_actual = scalers[target_column].inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "        y_pred = scalers[target_column].inverse_transform(y_pred).flatten()\n",
    "\n",
    "        kb2_rss_values = cross_config_df['2mb_rss'].values[5:5+len(y_actual)]\n",
    "        time_values = cross_config_df['time_sec'].values[5:5+len(y_actual)]\n",
    "\n",
    "        print(\"\\n=== Evaluating Performance ===\")\n",
    "        metrics = visualize_predictions(\n",
    "            actual_values=y_actual,\n",
    "            predicted_values=y_pred,\n",
    "            kb2_rss_values=kb2_rss_values,\n",
    "            time_steps=time_values,\n",
    "            save_path='output/evaluation_prediction_comparison.png'\n",
    "        )\n",
    "\n",
    "        with open('output/evaluation_metrics.txt', 'w') as f:\n",
    "            for k, v in metrics.items():\n",
    "                f.write(f\"{k}: {v}\\n\")\n",
    "        print(\"Evaluation metrics saved to output/evaluation_metrics.txt\")\n",
    "\n",
    "        print(\"\\n=== Evaluation Complete ===\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.4f}, MAPE: {metrics['mape']:.2f}%\")\n",
    "        return model, metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kb2_rss_path = \"data/curated/mm_rss_stat/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\"\n",
    "    kb4_rss_path = \"data/curated/mm_rss_stat/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\"\n",
    "    kb2_dtlb_path = \"data/curated/dtlb_misses/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\"\n",
    "    kb2_itlb_path = \"data/curated/itlb_misses/ee6f283a-1848-46a8-a741-cf7cda9368e3.redis.parquet\"\n",
    "    kb4_dtlb_path = \"data/curated/dtlb_misses/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\"\n",
    "    kb4_itlb_path = \"data/curated/itlb_misses/386c4cbd-3fa6-408a-84ef-4f31422eef80.redis.parquet\"\n",
    "    model, metrics = main(kb2_rss_path, kb4_rss_path, kb2_dtlb_path, kb2_itlb_path, kb4_dtlb_path, kb4_itlb_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
